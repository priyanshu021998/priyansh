{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_basics_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyanshu021998/priyansh/blob/main/NLP_basics_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FcRSGD3rAJE"
      },
      "outputs": [],
      "source": [
        "# spacy: natural language processing #nltk \n",
        "\n",
        "# matplolib # seaborn # pandas \n",
        "# python \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# in colab --> spacy is preinstalled \n",
        "# install spacy --> first \n",
        "\n",
        "import spacy "
      ],
      "metadata": {
        "id": "PT1aFQVtrud5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spacy ---> language ---> read english \n",
        "\n",
        "# language : English \n",
        "\n",
        "#load the english language library\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm') # verb, fullstop--> "
      ],
      "metadata": {
        "id": "opM6tA3Ssv8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp('Tesla is looking to buy twitter for $53 billion. The deal did not take place.')\n",
        "\n",
        "# we are trying to teach machine--> Numericals--->  this sentence "
      ],
      "metadata": {
        "id": "FT3CA5NMttXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc: \n",
        "  print(token.text, token.pos_, token.dep_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZDlW-QLuhN0",
        "outputId": "f2abb585-1f77-4279-e399-d0e9a5132c79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla PROPN nsubj\n",
            "is AUX aux\n",
            "looking VERB ROOT\n",
            "to PART aux\n",
            "buy VERB xcomp\n",
            "twitter NOUN dobj\n",
            "for ADP prep\n",
            "$ SYM quantmod\n",
            "53 NUM compound\n",
            "billion NUM pobj\n",
            ". PUNCT punct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization \n",
        "\n",
        "for token in doc:\n",
        "  print(token.text) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5ssr9Uyvk93",
        "outputId": "3d90a046-15d3-460e-c11b-09d2dd27d1a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla\n",
            "is\n",
            "looking\n",
            "to\n",
            "buy\n",
            "twitter\n",
            "for\n",
            "$\n",
            "53\n",
            "billion\n",
            ".\n",
            "The\n",
            "deal\n",
            "did\n",
            "not\n",
            "take\n",
            "place\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc1 = nlp('You can send me mails on ankietj@gmail.com!')"
      ],
      "metadata": {
        "id": "nNGp2alHxgqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc1:\n",
        "  print(token.text) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSW1GhQvxpm1",
        "outputId": "d019fd14-640c-4140-e781-bb26393c8f78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You\n",
            "can\n",
            "send\n",
            "me\n",
            "mails\n",
            "on\n",
            "ankietj@gmail.com\n",
            "!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc2 = nlp(\"let's take another example. U.S is a country!\")\n",
        "\n",
        "# \n",
        "\n",
        "# how do I tell nlp that let's the is not a string end."
      ],
      "metadata": {
        "id": "BP8NElGJyZA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc2:\n",
        "  print(token.text) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PztAu9dkzZZH",
        "outputId": "bae0d872-afc9-4d0e-d1ab-a4fbb868442a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "let\n",
            "'s\n",
            "take\n",
            "another\n",
            "example\n",
            ".\n",
            "U.S\n",
            "is\n",
            "a\n",
            "country\n",
            "!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# another function ---> NLP \n",
        "\n",
        "len(doc2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWmyBA0Azzai",
        "outputId": "12bde5ab-abcc-4d34-b956-bd4762074387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# i  want you to extract the word 'take' from doc2--> 21:13--> indexing/ \n",
        "\n",
        "doc2[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yc_Zdfqp0Coy",
        "outputId": "2685d659-11c5-4e59-a760-54e2c2fe15a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "take"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# give me the words from 2 to 5 \n",
        "doc2[2:6]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkuV2H2T1mtm",
        "outputId": "1ff8f7ae-4421-486d-c9c0-a2b15a08542b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "take another example."
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# can you answer me this question : are teh tokems reassignable --> take---> taken \n",
        "\n",
        "# break-- 21:18--> 21:30 \n",
        "\n",
        "doc2[2] ='taken'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "9Fnp36Zv13YF",
        "outputId": "d9abfbc8-fd2a-46a4-9591-e20c60a51aff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-0bff104e8a2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# break-- 21:18--> 21:30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdoc2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'taken'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'spacy.tokens.doc.Doc' object does not support item assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc2[-4:-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Lv-KJyc5Vvi",
        "outputId": "e8b6e23d-68d9-4196-c357-f496e1d7ffa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "is a country"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# can you extract all teh 'stop words' from the tokesn "
      ],
      "metadata": {
        "id": "RDDnGTLT5fuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# how do I know --> do we have to hard code --> stop words "
      ],
      "metadata": {
        "id": "tV-QVmmL5szY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# can you remove the fullstop (.) and the ! from doc2\" --> for , if condition loop---> \n",
        "\n",
        "# 21:36--> 21:45 \n",
        "new_doc = []\n",
        "for i in doc2:\n",
        "    if token.text != '.':\n",
        "        new_doc.append(i)\n",
        "print(new_doc)\n",
        "\n",
        "# assignment "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dAgi6U95xqZ",
        "outputId": "a9b9f397-4e10-47b1-bd3c-64a3408e7d3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[let, 's, take, another, example, ., U.S, is, a, country, !]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc2:\n",
        "  print(token.is_stop)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EeWKSxs87Ns",
        "outputId": "5a76844f-e548-46a1-ae0c-260e96075df9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "True\n",
            "True\n",
            "True\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "True\n",
            "False\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_doc2= []\n",
        "for token in doc2:\n",
        "  print(token.is_punct)\n",
        "  if token.is_punct != True:\n",
        "      new_doc2.append(token)\n",
        "\n",
        "print(new_doc2)\n",
        "\n",
        "# true --> string--> True --> "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNbWnTDu9cru",
        "outputId": "4453ed40-1c2a-4eb2-f4a9-8fa735be2cfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "[let, 's, take, another, example, U.S, is, a, country]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_doc = []\n",
        "for token in doc2:\n",
        "    if token.text != '.':\n",
        "        new_doc.append(token.text)\n",
        "print(new_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_zhhyz4-EbF",
        "outputId": "e33f0b25-1030-4e39-b407-b15007d1ce0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['let', \"'s\", 'take', 'another', 'example', 'U.S', 'is', 'a', 'country', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_sentence =[] \n",
        "\n",
        "\n",
        "for word in doc2:\n",
        "  if word.text == \"!\":\n",
        "      print(\"skipping\")\n",
        "  elif word.text == \".\":\n",
        "      print(\"skipping\")\n",
        "  else:\n",
        "      filtered_sentence.append(word)\n",
        "\n",
        "print(filtered_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHJAXk3h-YJk",
        "outputId": "894f0cbb-08fc-42dc-c81a-a2619aef7b7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n",
            "skipping\n",
            "[let, 's, take, another, example, U.S, is, a, country]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove teh punct from the doc ---> please send me the code \n",
        "\n",
        "new_doc2= []\n",
        "for token in doc:\n",
        "  print(token.is_punct)\n",
        "  if token.is_punct != True:\n",
        "      new_doc2.append(token)\n",
        "\n",
        "print(new_doc2)\n",
        "\n",
        "# true --> string--> True --> "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W10RyQ0y_Ent",
        "outputId": "4be09585-7757-4e9b-b423-2a4bebca915c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "[Tesla, is, looking, to, buy, twitter, for, $, 53, billion, The, deal, did, not, take, place]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I want you to write a code ---> which removes all teh currencty from teh tokes ---> doc --> spacy --> currency\n",
        "\n",
        "# 22:01--> 22:06 \n",
        "\n",
        "new_doc2= []\n",
        "for token in doc:\n",
        "  #print(token.is_punct)\n",
        "  if token.is_currency != True:\n",
        "      new_doc2.append(token)\n",
        "\n",
        "print(new_doc2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_As7G5w_c7V",
        "outputId": "ed1d829f-6f7b-4d29-a875-375bfc703ef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Tesla, is, looking, to, buy, twitter, for, 53, billion, ., The, deal, did, not, take, place, .]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# token ---> is_punct---> is_currency ---> \n",
        "\n",
        "# stopwords in NLP \n",
        "\n",
        "# in english---> 'the' 'is' 'and' ===? \n",
        "\n",
        "for token in doc2:\n",
        "  print(token.is_stop)"
      ],
      "metadata": {
        "id": "zdMS-SaxBdfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = nlp.Defaults.stop_words \n",
        "print(stopwords)\n",
        "len(stopwords)\n",
        "\n",
        "# 326 words --> tokens already found out as stopwords "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "493jlLykCGVd",
        "outputId": "5c7376fe-da1c-4182-8bff-842139ecff30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'’m', 'over', '’ll', 'elsewhere', 'keep', 'hundred', 'whereupon', 'nothing', 'fifteen', 'becomes', 'from', 'during', '‘re', 're', 'everyone', 'hereby', 'beyond', 'thereafter', 'upon', 'really', 'any', '’ve', 'twelve', 'somehow', 'an', 'top', 'moreover', 'never', '‘ll', 'why', 'regarding', 'himself', 'though', 'eight', 'because', 'me', 'on', 'five', 'take', 'anyhow', 'ten', 'ourselves', 'part', 'below', 'thence', 'did', 'along', 'another', 'someone', 'myself', 'behind', 'off', 'always', 'two', 'three', 'latterly', 'seems', 'just', 'could', 'anyone', 'whereas', 'about', 'rather', 'least', 'them', 'serious', 'via', 'still', 'above', 'much', 'get', 'due', 'are', 'therein', 'besides', 'others', 'whole', 'whenever', 'herself', '’re', 'ours', 'i', 'otherwise', 'around', 'often', 'sometimes', 'somewhere', 'hereupon', 'my', 'towards', 'him', 'after', 'amount', 'across', 'four', 'against', 'no', 'ever', 'doing', 'amongst', 'say', 'except', 'you', 'own', 'must', 'meanwhile', 'our', 'six', 'wherein', \"'ll\", 'without', 'toward', 'afterwards', 'among', 'done', 'into', 'please', 'throughout', '‘ve', 'hers', '‘s', 'other', 'hereafter', 'for', 'nobody', 'onto', 'was', 'whereby', 'former', 'its', 'or', 'where', 'empty', 'beforehand', 'fifty', 'she', 'and', 'seeming', 'move', 'mostly', 'within', 'side', 'although', 'enough', 'same', 'none', 'herein', 'this', 'ca', 'nowhere', 'us', 'something', 'everything', 'however', \"'s\", 'whereafter', 'see', 'at', 'every', 'back', 'their', 'itself', 'once', 'both', 'her', 'am', 'some', 'until', 'eleven', 'were', 'twenty', 'what', 'it', 'make', 'such', \"'m\", 'sixty', 'these', 'sometime', 'down', 'front', 'already', 'alone', 'do', 'even', 'become', 'which', 'there', 'wherever', 'n’t', 'further', 'be', 'quite', 'as', 'would', 'forty', 'thus', 'who', 'thereby', 'with', 'became', 'through', 'third', 'whether', 'beside', 'yours', 'per', 'so', 'n‘t', 'many', 'put', '’d', 'anywhere', 'therefore', 'most', 'whose', 'various', 'well', 'they', 'full', 'when', 'give', 'together', 'cannot', 'show', 'those', 'all', 'we', 'might', 'almost', 'each', 'nevertheless', 'under', 'a', 'nor', 'then', 'go', 'several', 'has', 'of', 'perhaps', 'mine', 'been', 'thru', 'very', 'here', 'call', 'name', 'your', '‘m', 'anything', 'is', 'used', 'first', 'seemed', \"'re\", 'yet', 'too', 'hence', 'he', 'seem', 'yourselves', 'either', 'the', 'out', 'whoever', 'since', 'not', 'latter', 'being', 'whither', 'in', 'have', 'few', 'but', 'will', 'to', 'anyway', \"n't\", 'whatever', 'only', '’s', 'yourself', 'using', 'everywhere', 'whence', 'between', \"'ve\", 'may', 'nine', 'that', 'namely', 'by', 'his', 'than', '‘d', 'does', 'last', 'indeed', 'up', 'if', \"'d\", 'bottom', 'next', 'can', 'unless', 'thereupon', 'again', 'how', 'themselves', 'less', 'also', 'else', 'before', 'more', 'formerly', 'becoming', 'one', 'should', 'whom', 'neither', 'noone', 'made', 'had', 'now', 'while'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "326"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = \"Welcome to the NLP module, This is your first class on NLP. You have learn't about tokenization and removing punt, $, etc, and stop words/.\"\n",
        "\n",
        "# the task ---> remove the punct, currency , stop words (# clearning --> stopwords--> relevance)---> send me the code \n",
        "\n",
        "# working break ---> 22:17---> 22:30 "
      ],
      "metadata": {
        "id": "sYsxN7bFClpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_sentence1 =[] \n",
        "\n",
        "text_data=nlp(\"Welcome to the NLP module, This is your first class on NLP. You have learn't about tokenization and removing punt, $, etc, and stop words/.\")\n",
        "for token in text_data:\n",
        "  if token.is_stop==True:\n",
        "    print(\"stopword token - \"+token.text)\n",
        "  elif token.is_punct==True:\n",
        "    print(\"punctuation token - \"+token.text)\n",
        "  elif token.is_currency ==True:\n",
        "    print(\"currency token - \"+ token.text)\n",
        "  else:\n",
        "    filtered_sentence1.append(token)\n",
        "\n",
        "print(filtered_sentence1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GT0nfPOpGx8_",
        "outputId": "c0e0557d-e6e4-4d13-cfa0-dfdf0eb0ecc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stopword token - to\n",
            "stopword token - the\n",
            "punctuation token - ,\n",
            "stopword token - This\n",
            "stopword token - is\n",
            "stopword token - your\n",
            "stopword token - first\n",
            "stopword token - on\n",
            "punctuation token - .\n",
            "stopword token - You\n",
            "stopword token - have\n",
            "stopword token - about\n",
            "stopword token - and\n",
            "punctuation token - ,\n",
            "currency token - $\n",
            "punctuation token - ,\n",
            "punctuation token - ,\n",
            "stopword token - and\n",
            "[Welcome, NLP, module, class, NLP, learn't, tokenization, removing, punt, etc, stop, words/.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_doc3= []\n",
        "for token in text_data:\n",
        "  #print(token.is_currency)\n",
        "  if token.is_currency != True:\n",
        "    if token.is_punct  != True:\n",
        "      if token.is_currency != True:\n",
        "        if token.is_stop != True:\n",
        "           new_doc3.append(token)\n",
        "\n",
        "print(new_doc3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxgTu2DVHOHd",
        "outputId": "ae64258c-6829-40c0-cd69-90f597c14239"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Welcome, NLP, module, class, NLP, learn't, tokenization, removing, punt, etc, stop, words/.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# in cleaning of this data---> /. from the token words/.  ---> assignment ---> Re (python in built package)"
      ],
      "metadata": {
        "id": "zJqyFtHfHcve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc4 = nlp(u'This is the first sentence. This is another sentence. This is the last sentence.')"
      ],
      "metadata": {
        "id": "-r79uo6KHzlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instead of tokems--->< three sentences ---> list fof all the sentences---> there is a spacy function \n",
        "\n",
        "#22:37--> 22:42 ---> \n",
        "\n",
        "doc4 = nlp(u'This is the first sentence. This is another sentence. This is the last sentence.')\n",
        "list(doc4.sents)\n",
        "\n",
        "# that you are easily able to break your data into different sentences, tokens, etc. with spacy "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzvcdDe7H3ns",
        "outputId": "59f8c155-709b-41cd-b52a-007c58fc4286"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[This is the first sentence.,\n",
              " This is another sentence.,\n",
              " This is the last sentence.]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sent in doc4.sents:\n",
        "  print(sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GiWe9MoJe8Y",
        "outputId": "037d0270-e461-4945-fbe4-ae66b622857f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the first sentence.\n",
            "This is another sentence.\n",
            "This is the last sentence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# can you create tokens of two ? ---> tokens of 1 ---> tokens \n",
        "\n",
        "# BERT_-> NLP "
      ],
      "metadata": {
        "id": "m9i3uddDJly8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "K7bOD6iWKbpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_data=nlp(\"Welcome to the NLP module, This is your first class on NLP. You have learn't about tokenization and removing punt $, etc.\")"
      ],
      "metadata": {
        "id": "2Udb9brSKdjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a dataframe of all the atttributes of the text data \n",
        "\n",
        "def creating_dataframe(text_data):\n",
        "  df = pd.DataFrame()\n",
        "\n",
        "  for i, token in enumerate(text_data):\n",
        "    df.loc[i, 'Tokens'] = token.text\n",
        "    df.loc[i,'Stop words'] = token.is_stop\n",
        "    df.loc[i,'Punct'] = token.is_punct\n",
        "    df.loc[i,'Currency'] = token.is_currency\n",
        "    df.loc[i,'POS'] = token.pos_\n",
        "    df.loc[i,'Lemma'] = token.lemma_"
      ],
      "metadata": {
        "id": "j4MRs1_2Kgcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "creating_dataframe(doc)\n",
        "# you can also apply the fuinctions of pandas onto your text data \n",
        "\n",
        "# text data---> spacy--> which to understand english--> load---> token---> extract all the properties"
      ],
      "metadata": {
        "id": "8na7YiN9LNwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "G3u5U1t6NUOB",
        "outputId": "4b12ec32-e5bd-45fa-9b26-92429423935d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 864
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Tokens Stop words  Punct Currency    POS         Lemma\n",
              "0        Welcome      False  False    False   VERB       welcome\n",
              "1             to       True  False    False    ADP            to\n",
              "2            the       True  False    False    DET           the\n",
              "3            NLP      False  False    False  PROPN           NLP\n",
              "4         module      False  False    False   NOUN        module\n",
              "5              ,      False   True    False  PUNCT             ,\n",
              "6           This       True  False    False   PRON          this\n",
              "7             is       True  False    False    AUX            be\n",
              "8           your       True  False    False   PRON          your\n",
              "9          first       True  False    False    ADJ         first\n",
              "10         class      False  False    False   NOUN         class\n",
              "11            on       True  False    False    ADP            on\n",
              "12           NLP      False  False    False  PROPN           NLP\n",
              "13             .      False   True    False  PUNCT             .\n",
              "14           You       True  False    False   PRON           you\n",
              "15          have       True  False    False   VERB          have\n",
              "16       learn't      False  False    False   NOUN       learn't\n",
              "17         about       True  False    False    ADP         about\n",
              "18  tokenization      False  False    False   NOUN  tokenization\n",
              "19           and       True  False    False  CCONJ           and\n",
              "20      removing      False  False    False   VERB        remove\n",
              "21          punt      False  False    False   NOUN          punt\n",
              "22             $      False  False     True    SYM             $\n",
              "23             ,      False   True    False  PUNCT             ,\n",
              "24           etc      False  False    False      X           etc\n",
              "25             .      False   True    False  PUNCT             ."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a445a8bc-11ae-41ab-a000-ef5410702e65\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tokens</th>\n",
              "      <th>Stop words</th>\n",
              "      <th>Punct</th>\n",
              "      <th>Currency</th>\n",
              "      <th>POS</th>\n",
              "      <th>Lemma</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Welcome</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>VERB</td>\n",
              "      <td>welcome</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>to</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>ADP</td>\n",
              "      <td>to</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>DET</td>\n",
              "      <td>the</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NLP</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>NLP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>module</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>module</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>,</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>This</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>PRON</td>\n",
              "      <td>this</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>is</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>AUX</td>\n",
              "      <td>be</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>your</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>PRON</td>\n",
              "      <td>your</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>first</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>ADJ</td>\n",
              "      <td>first</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>class</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>class</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>on</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>ADP</td>\n",
              "      <td>on</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>NLP</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>NLP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>.</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>You</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>PRON</td>\n",
              "      <td>you</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>have</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>VERB</td>\n",
              "      <td>have</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>learn't</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>learn't</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>about</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>ADP</td>\n",
              "      <td>about</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>tokenization</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>tokenization</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>and</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>CCONJ</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>removing</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>VERB</td>\n",
              "      <td>remove</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>punt</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>punt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>$</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>SYM</td>\n",
              "      <td>$</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>,</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>etc</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>X</td>\n",
              "      <td>etc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>.</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a445a8bc-11ae-41ab-a000-ef5410702e65')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a445a8bc-11ae-41ab-a000-ef5410702e65 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a445a8bc-11ae-41ab-a000-ef5410702e65');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"go went gone cats and dogs.\")"
      ],
      "metadata": {
        "id": "YCo7ypCFNE2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc: \n",
        "  print(token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lE7JPXRNydoT",
        "outputId": "2a339188-f9e0-4bde-9c8d-c898349f5d6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "go\n",
            "went\n",
            "go\n",
            "cat\n",
            "and\n",
            "dog\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# assignment -- lemma--> stemming, send me the code to find the stem ? 20:27 \n",
        "# stemming-0- lemma---. nltk not in spacy \n",
        "import nltk\n",
        "\n",
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()\n",
        "tokens = ['compute', 'computer', 'computed', 'computing']\n",
        "for token in tokens:\n",
        "    print(token + ' --> ' + stemmer.stem(token))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PQeDSzmytqZ",
        "outputId": "f2f42f41-4a50-47be-89a5-4414ad603700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compute --> comput\n",
            "computer --> comput\n",
            "computed --> comput\n",
            "computing --> comput\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NER : named entity recognition \n",
        "\n",
        "from spacy import displacy "
      ],
      "metadata": {
        "id": "NquDczDZ0HPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(nlp(\"Tim Cook is the CEO of Apple.\"), style  = \"ent\", jupyter = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "TaQOoNiI1CmZ",
        "outputId": "53649f92-3ec7-49e8-e797-61194edf1e3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Tim Cook\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " is the CEO of \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Apple\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ".</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"The capital of India is New Delhi. We are learning NLP.\")\n",
        "\n",
        "for token in doc.ents: \n",
        "  print(token.text, token.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSTy7fnI1c-c",
        "outputId": "4b8c86a7-bef5-4d6a-c10c-ee2e6556a645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "India GPE\n",
            "New Delhi GPE\n",
            "NLP ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with differnt examples ---> apply this with 5 examples and send me your inferences--> accurate / not accurate\n",
        "\n",
        "# time upto 20:45"
      ],
      "metadata": {
        "id": "JdsqUm_h2eQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"TajMahal is in one of the seven wonder. it is located in Agra.\")\n",
        "\n",
        "for token in doc.ents: \n",
        "  print(token.text, token.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwVx90o54GBS",
        "outputId": "e58e4337-82dc-4790-fd71-fc62e60ed330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TajMahal ORG\n",
            "one CARDINAL\n",
            "seven CARDINAL\n",
            "Agra PERSON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc.ents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PUzDm5k4VH_",
        "outputId": "fe1d3d39-d16e-4e19-870d-f562b1f23f61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(TajMahal, one, seven, Agra)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc2 = nlp(\"Mount Everest is the highest peak in the world.\")\n",
        "\n",
        "for token in doc2.ents: \n",
        "  print(token.text, token.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKFSLdOZ5CjW",
        "outputId": "7bac5b50-7e4b-4e98-c0f9-4fcad1049661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mount Everest LOC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OrvHjBxV6LTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = \"The Mars Orbiter Mission (MOM), informally known as Mangalyaan, was launched into Earth orbit on 5 November 2013 by the Indian Space Research Organisation (ISRO) and has entered Mars orbit on 24 September 2014. India thus became the first country to enter Mars orbit on its first attempt. It was completed at a record low cost of $74 million.\""
      ],
      "metadata": {
        "id": "huwlrCw06IDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc2 = nlp(text_data)\n",
        "\n",
        "for token in doc2.ents: \n",
        "  print(token.text, token.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnG3bITH6L18",
        "outputId": "fe1511e2-1137-419f-e6db-aa8c543981a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Mars Orbiter Mission ORG\n",
            "Mangalyaan PERSON\n",
            "Earth LOC\n",
            "5 November 2013 DATE\n",
            "the Indian Space Research Organisation ORG\n",
            "Mars LOC\n",
            "24 September 2014 DATE\n",
            "India GPE\n",
            "first ORDINAL\n",
            "Mars LOC\n",
            "first ORDINAL\n",
            "$74 million MONEY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(doc2, style = \"ent\", jupyter = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "-AQxJPii6ZR1",
        "outputId": "cfe5cf2b-570a-47aa-db7d-c1ab9fb445b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    The Mars Orbiter Mission\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " (MOM), informally known as \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Mangalyaan\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", was launched into \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Earth\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " orbit on \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    5 November 2013\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " by \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the Indian Space Research Organisation\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " (ISRO) and has entered \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Mars\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " orbit on \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    24 September 2014\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ". \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    India\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " thus became the \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    first\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
              "</mark>\n",
              " country to enter \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Mars\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " orbit on its \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    first\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
              "</mark>\n",
              " attempt. It was completed at a record low cost of \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    $74 million\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              ".</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# assignment--> can you extract all the LOC in the data \n",
        "\n",
        "new_doc2= []\n",
        "for token in doc2.ents:\n",
        "  #print(token.is_punct)\n",
        "  if token.label_== \"ORG\":\n",
        "      new_doc2.append(token)\n",
        "\n",
        "print(new_doc2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYGNTV2j7Amn",
        "outputId": "cbcb092c-1af6-4e40-e720-882a089110e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[The Mars Orbiter Mission, the Indian Space Research Organisation]\n"
          ]
        }
      ]
    }
  ]
}